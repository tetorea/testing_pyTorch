{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting CNN for an image classifier\n",
    "\n",
    "The aim is to detect the variety of the flower in the image amongst the varieties available in French Polynesia.<br>\n",
    "The image database is handmade (and thus quite small...) : Google + own pictures\n",
    "\n",
    "In order to test different architectures, the neural net definition is simplified using a list with keywords.<br>\n",
    "The neural net is then constructed automatically using this simplified format.\n",
    "\n",
    "Also I'm playing with a gaussian-based histogram layer in order to test its potential for CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NbEpochsMax = 3\n",
    "BatchSize = 4\n",
    "NumWorkers = 4\n",
    "InImgSize = 32\n",
    "LearningRate = 0.003\n",
    "TrainImgPath = \"./imagesFlower/train/\"\n",
    "TestImgPath = \"./imagesFlower/test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the mean and std values for normalization based on the training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgTransf = transforms.Compose([\n",
    "    transforms.Resize(InImgSize),\n",
    "    transforms.CenterCrop(InImgSize),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "imgSet = torchvision.datasets.ImageFolder(root=TrainImgPath, transform=imgTransf)\n",
    "imgLoader = data.DataLoader( imgSet, batch_size=BatchSize, shuffle=False,  num_workers=NumWorkers)\n",
    "\n",
    "image_means = torch.stack([sample.mean(1).mean(1) for sample, target in imgSet])\n",
    "image_means = image_means.mean(0)\n",
    "print( image_means )\n",
    "\n",
    "image_std = torch.stack([sample.view(sample.size(0),-1).std(1) for sample, target in imgSet])\n",
    "image_std = image_std.mean(0)\n",
    "print( image_std )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the images with the correct normalization and checking the available classes and number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgTransform = transforms.Compose([\n",
    "    transforms.Resize(InImgSize),\n",
    "    transforms.CenterCrop(InImgSize),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize( mean=image_means, std=image_std )\n",
    "    ])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=TrainImgPath, transform=imgTransform)\n",
    "trainloader = data.DataLoader(trainset, batch_size=BatchSize, shuffle=True,  num_workers=NumWorkers)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=TestImgPath, transform=imgTransform)\n",
    "testloader  = data.DataLoader(testset, batch_size=BatchSize, shuffle=True, num_workers=NumWorkers) \n",
    "\n",
    "print(\"Nb train samples: \", len(trainset))\n",
    "print(\"Nb test samples: \", len(testset))\n",
    "print(\"Detected Classes : \", trainset.class_to_idx)\n",
    "\n",
    "nbClasses = len( trainset.class_to_idx )\n",
    "classes = list( trainset.class_to_idx.keys() ) #('tiare', 'tipanie')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the min and max pixel values in the training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = -1.\n",
    "min_val = 1.\n",
    "for data in trainloader :\n",
    "    max_t = torch.max( data[0] )\n",
    "    min_t = torch.min( data[0] )\n",
    "    if max_t > max_val : max_val = max_t.item()\n",
    "    if min_t < min_val : min_val = min_t.item()\n",
    "print( \"max_val = \", max_val )\n",
    "print( \"min_val = \", min_val )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with a histogram-based layer.<br>\n",
    "To allow the gradient computation, it uses a gaussian based representation of the histogram bins.<br>\n",
    "The approach to use gaussians to approximate a discrete object in order to use the gradient was already used in my Humanoids'08 paper (A Next-Best-View Algorithm for Autonomous 3DObject Modeling by a Humanoid Robot) but the same approach has already been used for CNN with the Caffe framework in http://dde.binghamton.edu/download/model_hist/ <br>\n",
    "<br>\n",
    "...will need to tweak this a little more to make it really useful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianHistogram( nn.Module ):\n",
    "    def __init__( self, nb_input, nb_bin, min_v, max_v ):\n",
    "        super( GaussianHistogram, self ).__init__()\n",
    "        self.nb_bin = nb_bin\n",
    "        \n",
    "        # gaussian parameters\n",
    "        self.means = torch.Tensor( nb_bin )  # centers of each gaussian for each histogram bin\n",
    "        interval = ( float(max_v) - float(min_v) ) / float( nb_bin )\n",
    "        for i in range( nb_bin ):\n",
    "            self.means[i] = float(min_v) + float(i)*interval + interval / 2.\n",
    "\n",
    "        self.neg_inv_sq_sigma = -1.0 / (interval / 10.0)  # -1 / sigma²\n",
    "\n",
    "        # each histogram bin has a parameter to influence its importance, the optimizer will look for the best weights..\n",
    "        self.bin_weights = nn.Parameter( torch.rand( nb_input, nb_bin, requires_grad = True ) )\n",
    "        \n",
    "\n",
    "    def forward( self, x ):\n",
    "        # x.size() -> [4,3,32,32] = batch_size, nb_channels, width, height\n",
    "        # print( \"Size X for hist : \", x.size() )\n",
    "        nb_pixels = float( x.size(2)*x.size(3) )\n",
    "        hist = torch.Tensor( x.size(0), x.size(1), self.nb_bin ).to(device)\n",
    "\n",
    "        for b, x1 in enumerate( x ) :\n",
    "            for c, x2 in enumerate( x1 ) :\n",
    "                for h in range( self.nb_bin ):\n",
    "                    hist[b,c,h] = ( torch.exp( ( x2.sub( self.means[h] ) ).pow(2) * self.neg_inv_sq_sigma ) ).sum()\n",
    "            hist[b] *= self.bin_weights / nb_pixels\n",
    "            \n",
    "        # print( \"hist weights : \", self.bin_weights )\n",
    "        # return [batch_size, nb_channels, nb_bin]\n",
    "        return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same principle but the histogram is computed for a moving window inside the image\n",
    "class MovingGaussianHistogram( nn.Module ):\n",
    "    def __init__( self, nb_input, nb_bin, min_v, max_v, kernel_size, img_size ):\n",
    "        super( MovingGaussianHistogram, self ).__init__()\n",
    "        self.nb_bin = nb_bin\n",
    "        self.kernel_size = kernel_size\n",
    "        self.img_size = img_size\n",
    "        self.nb_pixels = float( img_size*img_size )\n",
    "        \n",
    "        # gaussian parameters\n",
    "        self.means = torch.Tensor( nb_bin )  # centers of each gaussian for each histogram bin\n",
    "        interval = ( float(max_v) - float(min_v) ) / float( nb_bin )\n",
    "        for i in range( nb_bin ):\n",
    "            self.means[i] = float(min_v) + float(i)*interval + interval / 2.\n",
    "\n",
    "        self.neg_inv_sq_sigma = -1.0 / (interval / 10.0)  # -1 / sigma²\n",
    "\n",
    "        # each histogram bin has a parameter to influence its importance, the optimizer will look for the best weights..\n",
    "        self.bin_weights = nn.Parameter( torch.rand( nb_input, (img_size-kernel_size+1)*(img_size-kernel_size+1), nb_bin, requires_grad = True ) )\n",
    "\n",
    "        \n",
    "    def forward( self, x ):\n",
    "        # x.size() -> [4,3,32,32] = batch_size, nb_channels, width, height\n",
    "        # print( \"Size X for hist : \", x.size() )\n",
    "\n",
    "        # get all patches from the batch, input, ..., images\n",
    "        patches = x.unfold( len(x.shape)-2, self.kernel_size, 1).unfold( len(x.shape)-1, self.kernel_size, 1) \n",
    "        print( \"Size of patches : \", patches.size() )\n",
    "\n",
    "        # create the histogram space for all patches\n",
    "        newDim = patches.size()[:-2] + tuple( [self.nb_bin] )\n",
    "        hist = torch.Tensor( newDim ).to( device )\n",
    "        \n",
    "        for b, x1 in enumerate( patches ) :\n",
    "            for c, x2 in enumerate( x1 ) :\n",
    "                for w, x3 in enumerate( x2 ) :\n",
    "                    for h, x4 in enumerate( x3 ) :\n",
    "                        for h in range( self.nb_bin ):\n",
    "                            hist[b,c,w,h,h] = ( torch.exp( ( x4.sub( self.means[h] ) ).pow(2) * self.neg_inv_sq_sigma ) ).sum()\n",
    "            hist[b] *= self.bin_weights / self.nb_pixels\n",
    "            \n",
    "        # print( \"hist weights : \", self.bin_weights )\n",
    "        # return [batch_size, nb_channels, nb_bin]\n",
    "        return hist\n",
    "\n",
    "# NEED TESTING !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prototype faster we use a basic list to define a neural net architecture (basic CNN for now).\n",
    "Each element of the list represents a layer.\n",
    "Layers are defined in the order in which they are executed.\n",
    "Similar to nn.Sequential but slightly more compact (...but with less options now) and we don't have to compute all the numbers for input or output\n",
    "\n",
    "One layer is composed of a name and optionally some parameters.\n",
    "Currently available layers:\n",
    "- \"In\", number_of_input_for_the_next_layer. In Must be the first layer!\n",
    "- \"Conv2\", number_of_output, kernel_size\n",
    "- \"MaxPool2\", kernel_size\n",
    "- \"AvgPool2\", kernel_size\n",
    "- \"Drop\", probability\n",
    "- \"ReLu\"\n",
    "- \"View\"\n",
    "- \"Linear\", number_of_output\n",
    "- \"GaussHist\", number of bins, min pixel value, max pixel value\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestNet(nn.Module):\n",
    "    def __init__(self, netConfig ):\n",
    "        super(TestNet, self).__init__()\n",
    "        \n",
    "        self.netConfig = netConfig\n",
    "        imgw = InImgSize\n",
    "        imgh = InImgSize\n",
    "        self.func = nn.ModuleList()  # if declared as [], the parameters of the function are not stored and we get the error : optimizer got an empty parameter list\n",
    "        self.viewSize = None         # we consider only one view layer in the net, for now\n",
    "\n",
    "        currOutput = 0               # number of outputs in the last computed layer\n",
    "        viewMul = 1\n",
    "        \n",
    "        if netConfig[0][0] != \"In\":\n",
    "            print( \"Error : the first item in the net configuration must be In\")\n",
    "            return\n",
    "        # TODO : add more test to check consistency between layers\n",
    "        \n",
    "        # compute all necessary layers using the declared netConfig\n",
    "        for i,l in enumerate( netConfig ):\n",
    "            layerName = l[0]\n",
    "            param1 = None if len(l) < 2 else l[1]\n",
    "            if layerName == \"In\" and i==0 :\n",
    "                print( \"In : \", param1 )\n",
    "                currOutput = param1\n",
    "                self.func.append( None )    # not a Module, so it cannot be added in the List\n",
    "            elif layerName == \"GaussHist\" and len(l)>3 :\n",
    "                print( \"GaussHist : \", param1, \", \", l[2], \", \", l[3] )\n",
    "                self.func.append( GaussianHistogram( currOutput, param1, l[2], l[3] ) )\n",
    "                viewMul = currOutput\n",
    "                currOutput = l[1]\n",
    "            elif layerName == \"MovGaussHist\" and len(l)>4 :\n",
    "                # NEED TESTING !\n",
    "                print( \"MovGaussHist : \", param1, \", \", l[2], \", \", l[3], \", \", l[4], \", \", imgw )\n",
    "                self.func.append( MovingGaussianHistogram( currOutput, param1, l[2], l[3], l[4], imgw ) )\n",
    "                imgw = imgw - l[4] + 1\n",
    "                imgh = imgh - l[4] + 1\n",
    "                viewMul = imgw * imgh\n",
    "                currOutput = param1\n",
    "                # NEED TESTING !\n",
    "            elif layerName == \"Conv2\" and len(l)>2 :\n",
    "                print( \"Conv2 : \", currOutput,\",\", param1,\",\", l[2] )\n",
    "                # TODO : add handling of stride, dilation and padding\n",
    "                self.func.append( nn.Conv2d( currOutput, param1, l[2] ) )\n",
    "                currOutput = param1\n",
    "                imgw = imgw - l[2] + 1\n",
    "                imgh = imgh - l[2] + 1\n",
    "                viewMul = imgw * imgh\n",
    "            elif layerName == \"AvgPool2\" :\n",
    "                print( \"AvgPool2 : \", param1 )\n",
    "                # TODO : add handling of stride, dilation and padding\n",
    "                self.func.append( nn.AvgPool2d( param1, param1 ) )\n",
    "                imgw = imgw / 2\n",
    "                imgh = imgh / 2\n",
    "                viewMul = imgw * imgh\n",
    "            elif layerName == \"MaxPool2\" :\n",
    "                print( \"MaxPool2 : \", param1 )\n",
    "                # TODO : add handling of stride, dilation and padding\n",
    "                self.func.append( nn.MaxPool2d( param1, param1 ) )\n",
    "                imgw = imgw / 2\n",
    "                imgh = imgh / 2\n",
    "                viewMul = imgw * imgh\n",
    "            elif layerName == \"Drop\" :\n",
    "                print( \"Drop : \", param1 )\n",
    "                self.func.append( nn.Dropout( p=param1 ) )\n",
    "            elif layerName == \"Linear\" :\n",
    "                print( \"Linear : \", int(currOutput),\",\", int(param1) )\n",
    "                self.func.append( nn.Linear( int(currOutput), int(param1) ) )\n",
    "                currOutput = param1\n",
    "            elif layerName == \"ReLu\" :\n",
    "                print( \"ReLu\" )\n",
    "                self.func.append( None )    # not a Module, so it cannot be added in the List\n",
    "            elif layerName == \"View\" :\n",
    "                currOutput = currOutput * viewMul\n",
    "                print( \"View : \", -1, \",\", int(currOutput) )\n",
    "                self.viewSize = currOutput\n",
    "                self.func.append( None )    # not a Module, so it cannot be added in the List\n",
    "            else :\n",
    "                print( \"Unknown layer : \", layerName )\n",
    "                self.func.append( None )    # not a Module, so it cannot be added in the List\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i,layer in enumerate( self.netConfig ) :\n",
    "            if self.func[i] != None :\n",
    "                x = self.func[i]( x )\n",
    "            elif layer[0] == \"View\" :\n",
    "                x = x.view( -1, int(self.viewSize) )\n",
    "            elif layer[0] == \"ReLu\" :\n",
    "                x = F.relu( x )\n",
    "            elif layer[0] == \"In\" :\n",
    "                None\n",
    "            else :\n",
    "                print(\"Unknown Function..\")\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing different CNN architectures..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If possible, use the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a function to train and evaluate a neural net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_architecture = 0\n",
    "best_acc = 0.0\n",
    "best_loss = 999\n",
    "\n",
    "def test_architecture( archi, netConfig ):    \n",
    "    global best_architecture, best_acc, best_loss\n",
    "    net = TestNet( netConfig )\n",
    "    net.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam( net.parameters(), lr=LearningRate )\n",
    "    # optimizer = optim.SGD(net.parameters(), lr=LearningRate, momentum=0.9)\n",
    "    best_net_acc = 0.0\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range( NbEpochsMax ):\n",
    "\n",
    "        # training phase \n",
    "        mean_loss = 0.\n",
    "        nb_mean = 0\n",
    "        for i, data in enumerate( trainloader, 0 ):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # print( \"parameters before : \", list( net.parameters() ) )\n",
    "\n",
    "            outputs = net( inputs )\n",
    "            loss = criterion( outputs, labels )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss = loss.item()\n",
    "            mean_loss += running_loss\n",
    "            nb_mean += 1\n",
    "            \n",
    "            # print( \"parameters after : \", list( net.parameters() ) )\n",
    "            print('[Epoch %3d, %4d] loss: %.3f' % (epoch, i, running_loss) )\n",
    "\n",
    "        mean_loss /= float(nb_mean)\n",
    "        \n",
    "        # test phase\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data in testloader:\n",
    "                images, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = net( images )\n",
    "                _, predicted = torch.max( outputs.data, 1 )\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            acc = 100 * correct / total\n",
    "            if acc > best_net_acc :\n",
    "                best_net_acc = acc\n",
    "                \n",
    "            epoch_time = time.time() - start_time\n",
    "            print('[Epoch %3d] acc: %.3f -- time: %d' % (epoch, acc, epoch_time) )\n",
    "\n",
    "            if acc > best_acc or (acc == best_acc and mean_loss < best_loss):\n",
    "                print(\"Backing up the network...\")\n",
    "                best_acc = acc\n",
    "                best_loss = mean_loss\n",
    "                best_architecture = archi\n",
    "                torch.save( net, './TahitiFlowerBest.pt' )\n",
    "\n",
    "    computation_time = time.time() - start_time\n",
    "    print('Architecture {} training finished -- {:.0f}m {:.0f}s'.format( archi, computation_time // 60, computation_time % 60) )\n",
    "\n",
    "    return best_net_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launching the training for some architectures we picked!<br>\n",
    "All architectures will be tested and the best one will be saved and reloaded at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures = [\n",
    "    [[\"In\",3],\n",
    "    [\"GaussHist\",10,min_val,max_val],\n",
    "    [\"View\"],\n",
    "    [\"Linear\",60],\n",
    "    [\"ReLu\"],\n",
    "    [\"Linear\",30],\n",
    "    [\"ReLu\"],\n",
    "    [\"Linear\",len(classes)]],\n",
    "\n",
    "    [[\"In\",3],\n",
    "    [\"Conv2\",10,3],\n",
    "    [\"GaussHist\",10,min_val,max_val],\n",
    "    [\"View\"],\n",
    "    [\"Linear\",60],\n",
    "    [\"ReLu\"],\n",
    "    [\"Linear\",30],\n",
    "    [\"ReLu\"],\n",
    "    [\"Linear\",len(classes)]],\n",
    "    \n",
    "    [[\"In\",3],\n",
    "    [\"Conv2\",10,3],\n",
    "    [\"ReLu\"],\n",
    "    [\"MaxPool2\",2],\n",
    "    [\"GaussHist\",10,0.,max_val],\n",
    "    [\"View\"],\n",
    "    [\"Linear\",60],\n",
    "    [\"ReLu\"],\n",
    "    [\"Linear\",30],\n",
    "    [\"ReLu\"],\n",
    "    [\"Linear\",len(classes)]],\n",
    "\n",
    "    [[\"In\",3],\n",
    "    [\"Conv2\",10,3],\n",
    "    [\"ReLu\"],\n",
    "    [\"MaxPool2\",2],\n",
    "    [\"GaussHist\",20,0.,max_val],\n",
    "    [\"View\"],\n",
    "    [\"Linear\",60],\n",
    "    [\"ReLu\"],\n",
    "    [\"Linear\",30],\n",
    "    [\"ReLu\"],\n",
    "    [\"Linear\",len(classes)]],\n",
    "\n",
    "    [[\"In\",3],\n",
    "    [\"Conv2\",10,3],\n",
    "    [\"ReLu\"],\n",
    "    [\"MaxPool2\",2],\n",
    "    [\"View\"],\n",
    "    [\"Linear\",60],\n",
    "    [\"ReLu\"],\n",
    "    [\"Linear\",30],\n",
    "    [\"ReLu\"],\n",
    "    [\"Linear\",len(classes)]],\n",
    "\n",
    "    [[\"In\",3],\n",
    "    [\"Conv2\",10,3],\n",
    "    [\"ReLu\"],\n",
    "    [\"MaxPool2\",2],\n",
    "    [\"View\"],\n",
    "    [\"Linear\",80],\n",
    "    [\"ReLu\"],\n",
    "    [\"Linear\",60],\n",
    "    [\"ReLu\"],\n",
    "    [\"Linear\",len(classes)]],\n",
    "\n",
    "    [[\"In\",3],\n",
    "    [\"Conv2\",10,3],\n",
    "    [\"ReLu\"],\n",
    "    [\"MaxPool2\",2],\n",
    "    [\"View\"],\n",
    "    [\"Linear\",40],\n",
    "    [\"ReLu\"],\n",
    "    [\"Linear\",30],\n",
    "    [\"ReLu\"],\n",
    "    [\"Linear\",20],\n",
    "    [\"ReLu\"],\n",
    "    [\"Linear\",len(classes)]],\n",
    "\n",
    "    [[\"In\",3],\n",
    "    [\"Conv2\",10,3],\n",
    "    [\"ReLu\"],\n",
    "    [\"MaxPool2\",2],\n",
    "    [\"Drop\",0.1],\n",
    "    [\"View\"],\n",
    "    [\"Linear\",50],\n",
    "    [\"ReLu\"],\n",
    "    [\"Linear\",40],\n",
    "    [\"ReLu\"],\n",
    "    [\"Linear\",len(classes)]],\n",
    "    \n",
    "    [[\"In\",3],\n",
    "    [\"Conv2\",10,5],\n",
    "    [\"ReLu\"],\n",
    "    [\"MaxPool2\",2],\n",
    "    [\"Conv2\",10,3],\n",
    "    [\"ReLu\"],\n",
    "    [\"MaxPool2\",2],\n",
    "    [\"View\"],\n",
    "    [\"Linear\",80],\n",
    "    [\"ReLu\"],\n",
    "    [\"Linear\",60],\n",
    "    [\"ReLu\"],\n",
    "    [\"Linear\",len(classes)]],\n",
    "]\n",
    "\n",
    "for archi, netConfig in enumerate(architectures):\n",
    "    test_architecture( archi, netConfig )\n",
    "\n",
    "print(\" --> The best handmade net : \", best_architecture )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we let Optuna find the architecture and hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_archi = len( architectures )\n",
    "\n",
    "def optunaObjective( trial ):\n",
    "    global num_archi\n",
    "\n",
    "    # construct the net architecture with the optimizer suggestions...\n",
    "    # many parameters could be added but the necessary trials increase exponentially so we need to keep it low\n",
    "    netConfig = [[\"In\",3]]\n",
    "\n",
    "    n_pre_layers = trial.suggest_int('n_pre_layers', 1, 10)\n",
    "    for i in range(n_pre_layers):\n",
    "        layerType = trial.suggest_categorical('layerType{}'.format(i), ['G','C','CR','CRM','D'])\n",
    "        if layerType == 'G':\n",
    "            # [\"GaussHist\",10,0.,max_val]\n",
    "            n_gauss_bin = trial.suggest_int('n_gauss_bin{}'.format(i), 5, 30) # TODO : compute the correct limits!\n",
    "            netConfig.append( [\"GaussHist\",n_gauss_bin,min_val,max_val] )\n",
    "\n",
    "        elif layerType == 'C':\n",
    "            # [\"Conv2\",10,5]\n",
    "            n_out_conv = trial.suggest_discrete_uniform('n_out_conv{}'.format(i),5,10,15,20)\n",
    "            n_ker_conv = trial.suggest_discrete_uniform('n_ker_conv{}'.format(i),3,5,7)\n",
    "            netConfig.append( [\"Conv2\",n_out_conv,n_ker_conv] )\n",
    "            \n",
    "        elif layerType == 'CR':\n",
    "            # [\"Conv2\",10,5]\n",
    "            # [\"ReLu\"]\n",
    "            n_out_conv = trial.suggest_discrete_uniform('n_out_conv{}'.format(i),5,10,15,20)\n",
    "            n_ker_conv = trial.suggest_discrete_uniform('n_ker_conv{}'.format(i),3,5,7)\n",
    "            netConfig.append( [\"Conv2\",n_out_conv,n_ker_conv] )\n",
    "            netConfig.append( [\"ReLu\"] )\n",
    "            \n",
    "        elif layerType == 'CRM':\n",
    "            # [\"Conv2\",10,5]\n",
    "            # [\"ReLu\"]\n",
    "            # [\"MaxPool2\",2]\n",
    "            n_out_conv = trial.suggest_discrete_uniform('n_out_conv{}'.format(i),5,10,15,20)\n",
    "            n_ker_conv = trial.suggest_discrete_uniform('n_ker_conv{}'.format(i),3,5,7)\n",
    "            n_ker_pool = trial.suggest_discrete_uniform('n_ker_pool{}'.format(i),2,3,4)\n",
    "            netConfig.append( [\"Conv2\",n_out_conv,n_ker_conv] )\n",
    "            netConfig.append( [\"ReLu\"] )\n",
    "            netConfig.append( [\"MaxPool2\",n_ker_pool] )\n",
    "\n",
    "        elif layerType == 'D':\n",
    "            # [\"Drop\",0.1],\n",
    "            n_drop = trial.suggest_discrete_uniform('n_drop{}'.format(i),0.1,0.2,0.3)\n",
    "            netConfig.append( [\"Drop\",n_drop] )\n",
    "\n",
    "    netConfig.append( [\"View\"] )\n",
    "    \n",
    "    n_lin_layers = trial.suggest_int('n_lin_layers', 1, 4)\n",
    "    for i in range(n_lin_layers-1):\n",
    "        numNeurons = trial.suggest_uniform('numNeurons{}'.format(i), 10, 60)\n",
    "        netConfig.append( [\"Linear\",numNeurons] )\n",
    "        netConfig.append( [\"ReLu\"] )\n",
    "        \n",
    "    netConfig.append( [\"Linear\",len(classes)] )\n",
    "    \n",
    "    try:\n",
    "        res = test_architecture( num_archi, netConfig )\n",
    "    except:\n",
    "        res = 0.0\n",
    "        # need many more coherence checks when constructing the architecture in order to avoid exceptions\n",
    "        \n",
    "    num_archi += 1\n",
    "    \n",
    "    return res\n",
    "\n",
    "'''\n",
    "\n",
    "study = optuna.create_study( direction='maximize' )\n",
    "study.optimize( optunaObjective, n_trials=100 )\n",
    "\n",
    "print( \"best param : \", study.best_params )\n",
    "print( \"best value : \", study.best_value )\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reloading the best net : \", best_architecture )\n",
    "net = torch.load( './TahitiFlowerBest.pt' )\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show images in a batch and the correct labels for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow( img ):\n",
    "    # need to put color channel at the end for matplotlib\n",
    "    image = img.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Un-normalize\n",
    "    image = np.array(image_std) * image + np.array(image_means)\n",
    "    \n",
    "    plt.imshow( image )\n",
    "    plt.show()\n",
    "\n",
    "# show all images in a batch\n",
    "dataiter = iter( testloader )\n",
    "images, labels = dataiter.next()\n",
    "imshow( torchvision.utils.make_grid(images) )\n",
    "\n",
    "print( 'GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(BatchSize)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the predicted labels for the same images as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net( images.to(device) )\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(BatchSize)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the recognition performance of the network for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(nbClasses))\n",
    "class_total = list(0. for i in range(nbClasses))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(nbClasses):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(nbClasses):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting using the Optuna hyper-parameters optimizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References :\n",
    "- https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "- https://medium.com/datadriveninvestor/creating-a-pytorch-image-classifier-da9db139ba80\n",
    "- http://dde.binghamton.edu/download/model_hist/\n",
    "- https://optuna.org/\n",
    "- ...\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
